{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version == 1.13.1\n",
      "Gym version == 0.13.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "\n",
    "print(f\"Tensorflow version == {tf.__version__}\")\n",
    "print(f\"Gym version == {gym.__version__}\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from collections import deque\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers\n",
    "from helpers import prepro, discount_rewards \n",
    "#could use observationwrapper https://alexandervandekleut.github.io/gym-wrappers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the documentation for gym : https://gym.openai.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open AI gym Atari env: 0: 'NOOP', 2: 'UP', 3: 'DOWN'\n",
    "ACTIONS = [0, 2, 3]\n",
    "\n",
    "OBSERVATION_DIM = 80 * 80\n",
    "\n",
    "MEMORY_CAPACITY = int(10e3)#int(10e5)\n",
    "ROLLOUT_SIZE = 10000\n",
    "\n",
    "\n",
    "# MEMORY stores tuples:\n",
    "# (observation, label, reward)\n",
    "MEMORY = deque([], maxlen=MEMORY_CAPACITY)\n",
    "def gen():\n",
    "    for m in list(MEMORY):\n",
    "        yield m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.hidden_dim = 20\n",
    "\n",
    "def build_graph(observations):\n",
    "    \"\"\"Calculates logits from the input observations tensor.\n",
    "    This function will be called twice: rollout and train.\n",
    "    The weights will be shared.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('model', reuse=tf.AUTO_REUSE):\n",
    "        hidden = tf.layers.dense(observations, 20, use_bias=False, activation=tf.nn.relu)\n",
    "        logits = tf.layers.dense(hidden, len(ACTIONS), use_bias=False)\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nenv = gym.make('Pong-v0')\\ninit_obs = env.reset()\\ninit_obs.shape\\nenv.close()\\n\\nobservations = tf.placeholder(shape=(None, OBSERVATION_DIM), dtype=tf.float32)\\noutput = build_graph(observations)\\nwith tf.Session() as sess:\\n    sess.run(init)\\n    logits = (sess.run(output, feed_dict= {observations:prepro(init_obs).reshape(-1,6400)}) )\\n    sample_action = tf.squeeze(tf.random.categorical(logits=logits, num_samples=10))\\n    print (sess.run(sample_action) )\\n\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "env = gym.make('Pong-v0')\n",
    "init_obs = env.reset()\n",
    "init_obs.shape\n",
    "env.close()\n",
    "\n",
    "observations = tf.placeholder(shape=(None, OBSERVATION_DIM), dtype=tf.float32)\n",
    "output = build_graph(observations)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    logits = (sess.run(output, feed_dict= {observations:prepro(init_obs).reshape(-1,6400)}) )\n",
    "    sample_action = tf.squeeze(tf.random.categorical(logits=logits, num_samples=10))\n",
    "    print (sess.run(sample_action) )\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE =1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\nabla _ { \\theta } J ( \\theta ) = \\nabla _ { \\theta } \\int \\pi _ { \\theta } ( \\tau ) r ( \\tau ) d \\tau$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-ad1dbae640f6>:9: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /Users/imran/Documents/testing/venv/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-8-266035ca4c2c>:11: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.random.categorical instead.\n",
      "WARNING:tensorflow:From /Users/imran/Documents/testing/venv/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:429: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, use\n",
      "    tf.py_function, which takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    \n",
      "WARNING:tensorflow:From /Users/imran/Documents/testing/venv/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Number of trainable variables: 2\n",
      "[<tf.Variable 'model/dense/kernel:0' shape=(6400, 20) dtype=float32_ref>, <tf.Variable 'model/dense_1/kernel:0' shape=(20, 3) dtype=float32_ref>]\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default() as g:\n",
    "    # rollout subgraph\n",
    "    with tf.name_scope('rollout'):\n",
    "        observations = tf.placeholder(shape=(None, OBSERVATION_DIM), dtype=tf.float32)\n",
    "\n",
    "        logits = build_graph(observations)\n",
    "\n",
    "        logits_for_sampling = tf.reshape(logits, shape=(1, len(ACTIONS)))\n",
    "\n",
    "        # Sample the action to be played during rollout.\n",
    "        sample_action = tf.squeeze(tf.multinomial(logits=logits_for_sampling, num_samples=1))\n",
    "\n",
    "    optimizer = tf.train.RMSPropOptimizer(\n",
    "        learning_rate=1e-3,\n",
    "        decay=0.99\n",
    "    )\n",
    "\n",
    "    # dataset subgraph for experience replay\n",
    "    with tf.name_scope('dataset'):\n",
    "        # the dataset reads from MEMORY\n",
    "        ds = tf.data.Dataset.from_generator(gen, output_types=(tf.float32, tf.int32, tf.float32))\n",
    "        ds = ds.shuffle(MEMORY_CAPACITY).repeat().batch(BATCH_SIZE)\n",
    "        iterator = ds.make_one_shot_iterator()\n",
    "\n",
    "    # training subgraph\n",
    "    with tf.name_scope('train'):\n",
    "        # the train_op includes getting a batch of data from the dataset,\n",
    "        #so we do not need to use a feed_dict when running the train_op.\n",
    "        \n",
    "        next_batch = iterator.get_next()\n",
    "        train_observations, labels, processed_rewards = next_batch\n",
    "\n",
    "        # This reuses the same weights in the rollout phase.\n",
    "        train_observations.set_shape((BATCH_SIZE, OBSERVATION_DIM))\n",
    "        train_logits = build_graph(train_observations)\n",
    "\n",
    "        cross_entropies = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=train_logits,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        probs = tf.nn.softmax(logits=train_logits)\n",
    "\n",
    "        loss = tf.reduce_sum(processed_rewards * cross_entropies)\n",
    "\n",
    "        global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "        train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "    print('Number of trainable variables: {}'.format(len(tf.trainable_variables())))\n",
    "    print(tf.trainable_variables())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /Users/imran/Documents/testing/venv/lib/python3.7/site-packages/agents/scripts/networks.py:42: RegisterKL.__init__ (from tensorflow.python.ops.distributions.kullback_leibler) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 6000\n",
    "from agents.tools.wrappers import AutoReset, FrameHistory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "RENDER =False\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>> epoch 1\n",
      ">>> Rollout phase\n",
      ">>> Train phase\n",
      "rollout reward: -20.612121310000003\n",
      ">>>>>>> epoch 2\n",
      ">>> Rollout phase\n",
      ">>> Train phase\n",
      "rollout reward: -20.65972784824753\n",
      ">>>>>>> epoch 3\n",
      ">>> Rollout phase\n",
      ">>> Train phase\n",
      "rollout reward: -20.374635796194426\n",
      ">>>>>>> epoch 4\n",
      ">>> Rollout phase\n",
      ">>> Train phase\n",
      "rollout reward: -20.526501094358554\n",
      ">>>>>>> epoch 5\n",
      ">>> Rollout phase\n",
      ">>> Train phase\n",
      "rollout reward: -20.516395557150478\n",
      ">>>>>>> epoch 6\n",
      ">>> Rollout phase\n",
      ">>> Train phase\n",
      "rollout reward: -20.60616476474297\n",
      ">>>>>>> epoch 7\n",
      ">>> Rollout phase\n",
      ">>> Train phase\n",
      "rollout reward: -20.407275055079218\n",
      ">>>>>>> epoch 8\n",
      ">>> Rollout phase\n",
      ">>> Train phase\n",
      "rollout reward: -20.4058121119963\n",
      ">>>>>>> epoch 9\n",
      ">>> Rollout phase\n",
      ">>> Train phase\n",
      "rollout reward: -20.23980849763526\n",
      ">>>>>>> epoch 10\n",
      ">>> Rollout phase\n",
      ">>> Train phase\n",
      "rollout reward: -20.246785104911343\n",
      ">>>>>>> epoch 11\n",
      ">>> Rollout phase\n",
      ">>> Train phase\n",
      "rollout reward: -20.27701158558075\n",
      ">>>>>>> epoch 12\n",
      ">>> Rollout phase\n",
      ">>> Train phase\n",
      "rollout reward: -20.63563309438263\n",
      ">>>>>>> epoch 13\n",
      ">>> Rollout phase\n",
      ">>> Train phase\n",
      "rollout reward: -20.38483851472256\n",
      ">>>>>>> epoch 14\n",
      ">>> Rollout phase\n",
      ">>> Train phase\n",
      "rollout reward: -20.211111168592414\n",
      ">>>>>>> epoch 15\n",
      ">>> Rollout phase\n",
      ">>> Train phase\n",
      "rollout reward: -20.360409225743826\n",
      ">>>>>>> epoch 16\n",
      ">>> Rollout phase\n"
     ]
    }
   ],
   "source": [
    "inner_env = gym.make('Pong-v0')\n",
    "# tf.agents helper to more easily track consecutive pairs of frames\n",
    "env = FrameHistory(inner_env, past_indices=[0, 1], flatten=False)\n",
    "# tf.agents helper to automatically reset the environment\n",
    "env = AutoReset(env)\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "    # lowest possible score after an episode as the\n",
    "    # starting value of the running reward\n",
    "    _rollout_reward = -21.0\n",
    "\n",
    "    for i in range(n_epoch):\n",
    "        print('>>>>>>> epoch {}'.format(i+1))\n",
    "\n",
    "        print('>>> Rollout phase')\n",
    "        epoch_memory = []\n",
    "        episode_memory = []\n",
    "\n",
    "        # The loop for actions/stepss\n",
    "        _observation = np.zeros(OBSERVATION_DIM)\n",
    "        while True:\n",
    "            # sample one action with the given probability distribution\n",
    "            _label = sess.run(sample_action, feed_dict={observations: [_observation]})\n",
    "\n",
    "            _action = ACTIONS[_label]\n",
    "\n",
    "            _pair_state, _reward, _done, _ = env.step(_action)\n",
    "\n",
    "            if RENDER:\n",
    "                env.render()\n",
    "\n",
    "            # record experience\n",
    "            episode_memory.append((_observation, _label, _reward))\n",
    "\n",
    "            # Get processed frame delta for the next step\n",
    "            pair_state = _pair_state\n",
    "\n",
    "            current_state, previous_state = pair_state\n",
    "            current_x = prepro(current_state)\n",
    "            previous_x = prepro(previous_state)\n",
    "\n",
    "            _observation = current_x - previous_x\n",
    "            \n",
    "            if _done:\n",
    "                obs, lbl, rwd = zip(*episode_memory)\n",
    "\n",
    "                # processed rewards\n",
    "                prwd = discount_rewards(rwd, gamma)\n",
    "                prwd -= np.mean(prwd)\n",
    "                prwd /= np.std(prwd)\n",
    "\n",
    "                # store the processed experience to memory\n",
    "                epoch_memory.extend(zip(obs, lbl, prwd))\n",
    "\n",
    "                # calculate the running rollout reward\n",
    "                _rollout_reward = 0.9 * _rollout_reward + 0.1 * sum(rwd)\n",
    "\n",
    "                episode_memory = []\n",
    "\n",
    "\n",
    "            #print(f'len(epoch_memory) : {len(epoch_memory)}')\n",
    "            #print(f'ROLLOUT_SIZE : {ROLLOUT_SIZE}')\n",
    "            #print(f'epoch : {i}')\n",
    "            \n",
    "            if len(epoch_memory) >= ROLLOUT_SIZE:\n",
    "                break\n",
    "\n",
    "        # add to the global memory\n",
    "        MEMORY.extend(epoch_memory)\n",
    "\n",
    "        print('>>> Train phase')\n",
    "        print(f'rollout reward: {_rollout_reward}')\n",
    "\n",
    "        # Here we train only once.\n",
    "        _, _global_step = sess.run([train_op, global_step])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "env = gym.make('Pong-v0')\n",
    "\n",
    "env.reset()\n",
    "for _ in range(50):\n",
    "\n",
    "    obs, rew, done, info = env.step(env.action_space.sample()) # take a random action\n",
    "    #print(obs, rew, done, info)\n",
    "    \n",
    "    plt.imshow(obs, interpolation='nearest')\n",
    "    plt.show()\n",
    "    #time.sleep(0.01)\n",
    "    #plt.pause(0.01)\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    if done:\n",
    "        env.reset()\n",
    "env.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"Color Image\",obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(obs, interpolation='nearest') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydata",
   "language": "python",
   "name": "pydata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
